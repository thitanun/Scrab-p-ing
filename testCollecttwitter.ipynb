{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Teaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Teaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "nltk.download('punkt')   \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as nltk_wt\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime,timedelta ,date\n",
    "# from pythainlp.corpus import thai_stopwords\n",
    "# from pythainlp import word_tokenize as pyt_wt\n",
    "from pythainlp import word_tokenize as pyt_wt\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'gOoM90Ig2rs8POa8wxzPgw0oD'\n",
    "consumer_secret= '74vY9D6ZmoMkO4zFFv0uZL9Sl3u2Y7UzibGZqtK1j5bN6agxU5'\n",
    "access_token= '1503736251530170368-rC1gNRmBIgs7HGdgSYuokoUYRKLij6'\n",
    "access_token_secret= '9I8ndE2w6jPVpod5n0YNNS7732VU4stmL8MAjHvcso5xT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClawTwitter:\n",
    "    def __init__(self, keyw:str):\n",
    "        self.listdata = []\n",
    "        self.keyword = keyw\n",
    "        self.data = self.get_datatweet()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "            api = tw.API(auth)\n",
    "            return api\n",
    "        except:\n",
    "            print(\"Error\")\n",
    "            exit(1)\n",
    "\n",
    "    def remove_url(txt):\n",
    "        \"\"\"Replace URLs found in a text string with nothing \n",
    "        (i.e. it will remove the URL from the string).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        txt : string\n",
    "            A text string that you want to parse and remove urls.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        The same txt string with url's removed.\n",
    "        \"\"\"\n",
    "\n",
    "        return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "    def remove_url_th(txt):\n",
    "        \"\"\"Replace URLs found in a text string with nothing \n",
    "        (i.e. it will remove the URL from the string).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        txt : string\n",
    "            A text string that you want to parse and remove urls.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        The same txt string with url's removed.\n",
    "        \"\"\"\n",
    "\n",
    "        return \" \".join(re.sub(\"([^\\u0E00-\\u0E7Fa-zA-Z' ]|^'|'$|''|(\\w+:\\/\\/\\S+))\", \"\", txt).split())\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'(@[A-Za-z0-9_]+)', '', text)\n",
    "        text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text_tokens = nltk_wt(text)\n",
    "        text = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        text = ' '.join(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def clean_text_th(self, text):\n",
    "        text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "        url = \"https://api.aiforthai.in.th/textcleansing\"\n",
    "        params = {f'text':{text}}\n",
    "        headers = {\n",
    "            'Apikey': \"DXApQFOxFoYXJWd2i0QoCRis4YJfp7az\",\n",
    "            }\n",
    "        res = requests.request(\"GET\", url, headers=headers, params=params)\n",
    "        # print('test sen', res.json())\n",
    "        return res.json()['cleansing_text']\n",
    "\n",
    "\n",
    "    def stemmer(self, text):\n",
    "        porter_text = PorterStemmer()\n",
    "        token_words = nltk_wt(text)\n",
    "        stem_word = []\n",
    "        for word in token_words:\n",
    "            stem_word.append(porter_text.stem(word))\n",
    "        return \" \".join(stem_word)\n",
    "\n",
    "\n",
    "    def sentiment(self, text_en):\n",
    "        sentiment_text = text_en.sentiment.polarity\n",
    "        if  sentiment_text > 0:\n",
    "            return 'positive'\n",
    "        elif sentiment_text < 0:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "\n",
    "    def sentiment_th(self, text_th):\n",
    "        url = \"https://api.aiforthai.in.th/ssense\"\n",
    "        params = {'text':text_th}\n",
    "        headers = {\n",
    "            'Apikey': \"DXApQFOxFoYXJWd2i0QoCRis4YJfp7az\"\n",
    "            }\n",
    "        res = requests.get(url, headers=headers, params=params)\n",
    "        if res.json()['sentiment']['polarity'] == '':\n",
    "            polarity = \"neutral\"\n",
    "        else:\n",
    "            polarity = res.json()['sentiment']['polarity']\n",
    "        # print('hey', res.json())\n",
    "        return polarity\n",
    "\n",
    "\n",
    "    def get_datatweet(self):\n",
    "        api = self.connect()\n",
    "        today = datetime.utcnow().date()\n",
    "        today_str = today.strftime(\"%Y-%m-%d\")\n",
    "        tweets = tw.Cursor(api.search_tweets,\n",
    "                    q=self.keyword + \" -filter:retweets\",\n",
    "                    since= today_str).items(100)\n",
    "                    # since= today_str).items(10)\n",
    "                     \n",
    "        tweet_list = []    \n",
    "        for tweet in tweets:\n",
    "            tweet_list.append(tweet)\n",
    "        tweets = tweet_list\n",
    "\n",
    "        data_tweet = [[self.keyword,\n",
    "                        tweet.user.screen_name,\n",
    "                        # tweet.lang,\n",
    "                        tweet.user.location if tweet.user.location != '' else 'unknown',\n",
    "                        tweet.created_at.replace(tzinfo = None) + timedelta(hours = 7),\n",
    "                        ClawTwitter.remove_url(tweet.text) if tweet.lang == \"en\" else ClawTwitter.remove_url_th(tweet.text),\n",
    "                        tweet.retweet_count,\n",
    "                        tweet.favorite_count,\n",
    "                        # self.sentiment(TextBlob(self.stemmer(self.clean_text(ClawTwitter.remove_url(tweet.text))))),\n",
    "                        # self.sentiment(TextBlob(self.stemmer(ClawTwitter.remove_url(tweet.text)))),\n",
    "                        self.sentiment(TextBlob(self.stemmer(self.clean_text(ClawTwitter.remove_url(tweet.text)))))\n",
    "                                        if tweet.lang == \"en\" else self.sentiment_th(self.clean_text_th(tweet.text)),\n",
    "                        # self.sentiment_th(tweet.text) if tweet.lang == \"th\"\n",
    "                        #             else self.sentiment(TextBlob(self.stemmer(self.clean_text(ClawTwitter.remove_url(tweet.text))))),\n",
    "                        f\"https://twitter.com/twitter/statuses/{tweet.id}\"] for tweet in tweets]\n",
    "\n",
    "        return data_tweet\n",
    "\n",
    "\n",
    "    def show_data(self, data_tweet:list):\n",
    "        tweet_frame = pd.DataFrame(data=data_tweet,\n",
    "                            # columns=['topic', 'user','language','location','post date', 'tweet','retweet','likes', 'sentiment','tweet link'])\n",
    "                            columns=['topic', 'user','location','post date', 'tweet','retweet','likes', 'sentiment','tweet link'])\n",
    "        \n",
    "        # tweet_frame.to_excel(f\"{self.keyword}.xlsx\", engine=\"openpyxl\", index=False)\n",
    "        return tweet_frame\n",
    "\n",
    "\n",
    "    def twitter_to_xlsx(self, tweets):\n",
    "        hashtag = self.keyword\n",
    "        today = date.today()\n",
    "        today_str = today.strftime(\"%d%m%Y\")\n",
    "        tweets.to_excel(hashtag+\"_\"+today_str+\".xlsx\", engine=\"openpyxl\", index=False)\n",
    "    \n",
    "\n",
    "    def data_hashtagcount(self):\n",
    "        api = self.connect()\n",
    "        today = datetime.utcnow().date()\n",
    "        today_str = today.strftime(\"%Y-%m-%d\")\n",
    "        new_column = []\n",
    "        for tweet in tw.Cursor(api.search_tweets,\n",
    "                    q=self.keyword + \" -filter:retweets\",\n",
    "                    since= today_str).items(100):\n",
    "            entity_ht = tweet.entities.get('hashtags')\n",
    "            hashtag = \"\"\n",
    "            for i in range(0, len(entity_ht)):\n",
    "                hashtag = hashtag + \",#\" + entity_ht[i]['text']\n",
    "            new_column.append([hashtag])\n",
    "        data_frame = pd.DataFrame(data = new_column, columns=[\"hashtag\"])\n",
    "        return data_frame\n",
    "    \n",
    "\n",
    "    def twitterhashtage_to_xlsx(self, datframe):\n",
    "        hashtag = self.keyword\n",
    "        today = date.today()\n",
    "        today_str = today.strftime(\"%d%m%Y\")\n",
    "        datframe.to_excel(hashtag+\"_\"+\"hashcount\"+\"_\"+today_str+\".xlsx\", engine=\"openpyxl\", index=False)\n",
    "\n",
    "\n",
    "    def slash_tokenize(self,data):  \n",
    "        result = data.split(\",\")\n",
    "        return result\n",
    "\n",
    "\n",
    "    def count_wordframe(self):\n",
    "        topic = self.keyword\n",
    "        today = date.today()\n",
    "        today_str = today.strftime(\"%d%m%Y\")\n",
    "        readfile = pd.read_excel(topic+\"_\"+\"hashcount\"+\"_\"+today_str+\".xlsx\")\n",
    "        hastag_data = readfile[\"hashtag\"].dropna()\n",
    "        vectorizer = CountVectorizer(tokenizer = self.slash_tokenize)\n",
    "        transformed_data = vectorizer.fit_transform(hastag_data)\n",
    "        hash_tag_cnt_df = pd.DataFrame(columns = ['word', 'count']) \n",
    "        hash_tag_cnt_df['word'] = vectorizer.get_feature_names()\n",
    "        hash_tag_cnt_df['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "        tophash_tag = hash_tag_cnt_df.sort_values(by=['count'], ascending=False).head(20)\n",
    "        return tophash_tag\n",
    "\n",
    "    def twittercount_to_xlsx(self, datframe):\n",
    "        hashtag = self.keyword\n",
    "        today = date.today()\n",
    "        today_str = today.strftime(\"%d%m%Y\")\n",
    "        datframe.to_excel(hashtag+\"_\"+\"count\"+\"_\"+today_str+\".xlsx\", engine=\"openpyxl\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize_en(self, d):\n",
    "        # print('hey', d)\n",
    "        d = re.sub('[ก-๙]', '', d)\n",
    "        d = re.sub('[0-9]', '', d)\n",
    "        # print('test d', d)\n",
    "        result = d.split(\" \")\n",
    "        result = list(filter(None, result))\n",
    "        return result\n",
    "\n",
    "    def count_word_en(self, dfr):\n",
    "        new_text = []\n",
    "        for txt in dfr[\"tweet\"]:\n",
    "            new_text.append(self.clean_text(txt))\n",
    "        \n",
    "        vectorizer = CountVectorizer(tokenizer=self.tokenize_en)\n",
    "        transformed_data = vectorizer.fit_transform(new_text)\n",
    "        keyword_df_en = pd.DataFrame(columns = ['word', 'count'])\n",
    "        keyword_df_en['word'] = vectorizer.get_feature_names()\n",
    "        keyword_df_en['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "        topword_en = keyword_df_en.sort_values(by=['count'], ascending=False).head(20)\n",
    "\n",
    "        return topword_en\n",
    "\n",
    "\n",
    "    # def tokenize_th(self, d):\n",
    "    #     # print('hey', d)\n",
    "    #     d = re.sub('[A-Za-z]', '', d)\n",
    "    #     d = re.sub('[0-9]', '', d)\n",
    "    #     # print('test d', d)\n",
    "    #     result = d.split(\" \")\n",
    "    #     # print('re', result)\n",
    "    #     result = list(filter(None, result))\n",
    "    #     return result\n",
    "\n",
    "    # # def test_th(self, data):\n",
    "    # #     # a = self.tokenize_th(data)\n",
    "    # #     url ='https://api.aiforthai.in.th/ssense'\n",
    "    # #     headers = {'Apikey':'DXApQFOxFoYXJWd2i0QoCRis4YJfp7az'}\n",
    "    # #     params = {'text': data }\n",
    "    # #     # res = requests.get(url, headers=headers, params=params)\n",
    "    # #     # # print(res.status_code)\n",
    "    # #     # # print(res.headers)\n",
    "    # #     # print(res.json())\n",
    "    # #     res = requests.request(\"GET\", url, headers=headers, params=params)\n",
    "    # #     # print(res.json())\n",
    "    # #     return res.json()['tokens']\n",
    "\n",
    "    # def count_word_th(self, dfr):\n",
    "    #     new_text = []\n",
    "    #     for txt in dfr[\"tweet\"]:\n",
    "    #         new_text.append(self.sentiment_th_test(self.clean_text_th(txt)))\n",
    "    #         # new_text.append(self.clean_text_th(txt))\n",
    "    #     # print('test new', new_text)\n",
    "        \n",
    "    #     vectorizer = CountVectorizer(tokenizer=self.tokenize_th)\n",
    "    #     transformed_data = vectorizer.fit_transform(new_text)\n",
    "    #     # count_data = zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))\n",
    "    #     keyword_df_th = pd.DataFrame(columns = ['word', 'count'])\n",
    "    #     keyword_df_th['word'] = vectorizer.get_feature_names()\n",
    "    #     keyword_df_th['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "    #     topword_th = keyword_df_th.sort_values(by=['count'], ascending=False).head(20)\n",
    "\n",
    "    #     return topword_th\n",
    "\n",
    "\n",
    "    # def sentiment_th_test(self, text_th):\n",
    "    #     # print('test', text_th)\n",
    "    #     url = \"https://api.aiforthai.in.th/ssense\"\n",
    "    #     params = {'text':text_th}\n",
    "    #     headers = {\n",
    "    #         'Apikey': \"DXApQFOxFoYXJWd2i0QoCRis4YJfp7az\"\n",
    "    #         }\n",
    "    #     res = requests.get(url, headers=headers, params=params)\n",
    "    #     # print('hey', res)\n",
    "    #     # print('hi', res.json())\n",
    "    #     # a = res.json()['preprocess']['segmented']\n",
    "    #     return res.json()['segmented']\n",
    "\n",
    "\n",
    "    # def tokenize(self, d):\n",
    "    #     result = d.split(\"/\")\n",
    "    #     result = list(filter(None, result))\n",
    "    #     return result\n",
    "\n",
    "    # def test_w(self, tweet_dataf):\n",
    "    #     new_text = []\n",
    "    #     for txt in tweet_dataf[\"tweet\"]:\n",
    "    #         new_text.append(self.sentiment_th_test(txt))\n",
    "    #     if txt.lang == \"th\":\n",
    "    #         vectorizer = CountVectorizer(tokenizer=self.tokenize)\n",
    "    #         transformed_data = vectorizer.fit_transform(new_text)\n",
    "    #         count_data = zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))\n",
    "    #         keyword_df2 = pd.DataFrame(columns = ['word', 'count'])\n",
    "    #         keyword_df2['word'] = vectorizer.get_feature_names()\n",
    "    #         keyword_df2['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "    #         keyword_df2.sort_values(by=['count'], ascending=False).head(10)\n",
    "    #     return keyword_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: since\n",
      "Unexpected parameter: since\n"
     ]
    }
   ],
   "source": [
    "# test = ClawTwitter(\"#REDVELVET\", \"th\")\n",
    "test = ClawTwitter(\"#แจยง\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>user</th>\n",
       "      <th>location</th>\n",
       "      <th>post date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>jyeverytime</td>\n",
       "      <td>ty jh /ดูกลอนที่เฟบ</td>\n",
       "      <td>2022-04-05 20:14:39</td>\n",
       "      <td>ต้องบำเพ็ญ บารมี ซักเท่าใดถึงได้ใจ แจฮยอน มาคร...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>https://twitter.com/twitter/statuses/151133148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>inqblessgod</td>\n",
       "      <td>milk</td>\n",
       "      <td>2022-04-05 18:43:53</td>\n",
       "      <td>อัปแล้วนะคะ ตอนที่เจ็ดของพี่โวลต์ แจยง jaeyong...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>https://twitter.com/twitter/statuses/151130864...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>Cakes_cooky</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2022-04-05 18:22:59</td>\n",
       "      <td>bubujy กี้ดดดดด JAEYONG แจยง</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>https://twitter.com/twitter/statuses/151130338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>mycutiebubuu</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2022-04-05 14:59:29</td>\n",
       "      <td>เค้าส่งต่อเล่ม ฿ รวมส่งค่ะ สนใจเดมมาขอดูได้นะค...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>https://twitter.com/twitter/statuses/151125217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>phabo88</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2022-04-05 12:01:55</td>\n",
       "      <td>ไม่น่ากลัว เราลืมคำว่าไม่ อบอุ่นหัวใจค่ะ ฟิคแจ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>https://twitter.com/twitter/statuses/151120748...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>phabo88</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2022-04-05 11:57:54</td>\n",
       "      <td>ตอนหน้าเตรียมกระดาษทิชชูเหมือนเดิม เราเอาเรื่อ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>https://twitter.com/twitter/statuses/151120647...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>inqblessgod</td>\n",
       "      <td>milk</td>\n",
       "      <td>2022-04-05 09:36:46</td>\n",
       "      <td>จีปเฮอดำเนินมาถึงตอนสุดท้ายแล้วนะ จบแล้วค่ะ แจ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>https://twitter.com/twitter/statuses/151117095...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic          user             location           post date  \\\n",
       "0  #แจยง   jyeverytime  ty jh /ดูกลอนที่เฟบ 2022-04-05 20:14:39   \n",
       "1  #แจยง   inqblessgod                 milk 2022-04-05 18:43:53   \n",
       "2  #แจยง   Cakes_cooky              unknown 2022-04-05 18:22:59   \n",
       "3  #แจยง  mycutiebubuu              unknown 2022-04-05 14:59:29   \n",
       "4  #แจยง       phabo88              unknown 2022-04-05 12:01:55   \n",
       "5  #แจยง       phabo88              unknown 2022-04-05 11:57:54   \n",
       "6  #แจยง   inqblessgod                 milk 2022-04-05 09:36:46   \n",
       "\n",
       "                                               tweet  retweet  likes  \\\n",
       "0  ต้องบำเพ็ญ บารมี ซักเท่าใดถึงได้ใจ แจฮยอน มาคร...        7      5   \n",
       "1  อัปแล้วนะคะ ตอนที่เจ็ดของพี่โวลต์ แจยง jaeyong...        1      0   \n",
       "2                       bubujy กี้ดดดดด JAEYONG แจยง        0      0   \n",
       "3  เค้าส่งต่อเล่ม ฿ รวมส่งค่ะ สนใจเดมมาขอดูได้นะค...        0      0   \n",
       "4  ไม่น่ากลัว เราลืมคำว่าไม่ อบอุ่นหัวใจค่ะ ฟิคแจ...        0      0   \n",
       "5  ตอนหน้าเตรียมกระดาษทิชชูเหมือนเดิม เราเอาเรื่อ...        1      1   \n",
       "6  จีปเฮอดำเนินมาถึงตอนสุดท้ายแล้วนะ จบแล้วค่ะ แจ...        1      2   \n",
       "\n",
       "  sentiment                                         tweet link  \n",
       "0  positive  https://twitter.com/twitter/statuses/151133148...  \n",
       "1   neutral  https://twitter.com/twitter/statuses/151130864...  \n",
       "2   neutral  https://twitter.com/twitter/statuses/151130338...  \n",
       "3  positive  https://twitter.com/twitter/statuses/151125217...  \n",
       "4   neutral  https://twitter.com/twitter/statuses/151120748...  \n",
       "5   neutral  https://twitter.com/twitter/statuses/151120647...  \n",
       "6   neutral  https://twitter.com/twitter/statuses/151117095...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.show_data(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = test.show_data(test.data)\n",
    "test.twitter_to_xlsx(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teaw\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jaeyong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>readawrite</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bubujy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  count\n",
       "1     jaeyong      3\n",
       "2  readawrite      2\n",
       "0      bubujy      1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data = test.show_data(test.data)\n",
    "test.count_word_en(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_data = test.show_data(test.data)\n",
    "# test.count_word_th(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: since\n",
      "Unexpected parameter: since\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,#แจยง,#jaeyong,#ฟิคแจยง,#readAwrite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,#JAEYONG,#แจยง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,#ออนซอนเดจย,#แจยง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,#ฟิคแจยง,#แจยง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,#แจยง,#jaeyong,#ฟิคแจยง,#readAwrite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                hashtag\n",
       "0                                      \n",
       "1  ,#แจยง,#jaeyong,#ฟิคแจยง,#readAwrite\n",
       "2                       ,#JAEYONG,#แจยง\n",
       "3                    ,#ออนซอนเดจย,#แจยง\n",
       "4                       ,#ฟิคแจยง,#แจยง\n",
       "5                                      \n",
       "6  ,#แจยง,#jaeyong,#ฟิคแจยง,#readAwrite"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.data_hashtagcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: since\n",
      "Unexpected parameter: since\n"
     ]
    }
   ],
   "source": [
    "hashtag_data = test.data_hashtagcount()\n",
    "test.twitterhashtage_to_xlsx(hashtag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teaw\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#แจยง</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#jaeyong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#ฟิคแจยง</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#readawrite</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#ออนซอนเดจย</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0                   5\n",
       "5        #แจยง      5\n",
       "1     #jaeyong      3\n",
       "3     #ฟิคแจยง      3\n",
       "2  #readawrite      2\n",
       "4  #ออนซอนเดจย      1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count_wordframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teaw\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "count_wordf = test.count_wordframe()\n",
    "test.twittercount_to_xlsx(count_wordf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>แจ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ยง</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jaeyong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>มา</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ค่ะ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>เรา</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ฟิค</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>ได้</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ไม่</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>เด</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>readawrite</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ตอน</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>เขียน</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>เขา</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>อ่อนไหว</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>เค้า</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>฿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ฮ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bubujy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>อัป</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "74          แจ      9\n",
       "37          ยง      9\n",
       "1      jaeyong      3\n",
       "35          มา      3\n",
       "9          ค่ะ      3\n",
       "66         เรา      3\n",
       "32         ฟิค      3\n",
       "81         ได้      2\n",
       "83         ไม่      2\n",
       "61          เด      2\n",
       "2   readawrite      2\n",
       "17         ตอน      2\n",
       "58       เขียน      1\n",
       "57         เขา      1\n",
       "54     อ่อนไหว      1\n",
       "59        เค้า      1\n",
       "56           ฿      1\n",
       "55           ฮ      1\n",
       "0       bubujy      1\n",
       "53         อัป      1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_th(d):\n",
    "    # print('hey', d)\n",
    "    d = re.sub('[a-z]', '', d)\n",
    "    d = re.sub('[A-Z]', '', d)\n",
    "    d = re.sub('[0-9]', '', d)\n",
    "    # print('test d', d)\n",
    "    result = d.split(\" \")\n",
    "    result = list(filter(None, result))\n",
    "    return result\n",
    "\n",
    "def clean_text_th(text):\n",
    "    # text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    # url = \"https://api.aiforthai.in.th/textcleansing\"\n",
    "    # params = {f'text':{text}}\n",
    "    # headers = {\n",
    "    #     'Apikey': \"REONBZlQhDNoH07AoLRqrOf4dhuAhj9C\",\n",
    "    #     }\n",
    "    # res = requests.request(\"GET\", url, headers=headers, params=params)\n",
    "    # # print(res.json())\n",
    "    # return res.json()['cleansing_text']\n",
    "    url ='https://api.aiforthai.in.th/lextoplus'\n",
    "    headers = {'Apikey':\"DXApQFOxFoYXJWd2i0QoCRis4YJfp7az\"}\n",
    "    \n",
    "    params = {'text':text,'norm':'1'}\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    return response.json()['tokens']\n",
    "\n",
    "def count_word_en(new_text):\n",
    "# def count_word_en(dfr):\n",
    "    # new_text = []\n",
    "    # for txt in dfr[\"tweet\"]:\n",
    "    #     new_text.append(test.clean_text(txt))\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=test.tokenize_en)\n",
    "    transformed_data = vectorizer.fit_transform(new_text)\n",
    "    keyword_df_en = pd.DataFrame(columns = ['word', 'count'])\n",
    "    keyword_df_en['word'] = vectorizer.get_feature_names_out()\n",
    "    keyword_df_en['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "    # topword_en = keyword_df_en.sort_values(by=['count'], ascending=False).head(20)\n",
    "\n",
    "    return keyword_df_en\n",
    "\n",
    "def count_word_th(new_text):\n",
    "    # new_text = []\n",
    "    # for txt in dfr[\"tweet\"]:\n",
    "    #     new_text.append(test.clean_text(txt))\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer = tokenize_th)\n",
    "    transformed_data = vectorizer.fit_transform(new_text)\n",
    "    keyword_df_th = pd.DataFrame(columns = ['word', 'count'])\n",
    "    keyword_df_th['word'] = vectorizer.get_feature_names_out()\n",
    "    keyword_df_th['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "    # topword_en = keyword_df_en.sort_values(by=['count'], ascending=False).head(20)\n",
    "\n",
    "    return keyword_df_th\n",
    "\n",
    "\n",
    "dfr = tweets_data\n",
    "new_text_th = []\n",
    "new_text_en = []\n",
    "test_th = [\"วันนี้ฝนไม่ตก\",\"พระอาทิตย์แจ่มใส\"]\n",
    "for txt in dfr[\"tweet\"]:\n",
    "# for txt in test :\n",
    "    new_text_en.append(test.clean_text(txt))\n",
    "    # for txt_cut in pyt_wt(txt, keep_whitespace=False,engine=\"multi_cut\"):\n",
    "    for txt_cut in pyt_wt(txt, keep_whitespace=False,engine=\"multi_cut\"):\n",
    "    # for txt_cut in clean_text_th(txt):\n",
    "    #     if txt_cut == '':\n",
    "    #         pass\n",
    "    #     else:\n",
    "            new_text_th.append(txt_cut)\n",
    "    \n",
    "    # new_text.append(clean_text_th((txt))) \n",
    "\n",
    "# for txt in dfr[\"tweet\"]:\n",
    "#     new_text_en.append(test.clean_text(txt))\n",
    "# new_text = new_text_th + new_text_en\n",
    "# vectorizer = CountVectorizer(tokenizer= tokenize_th)\n",
    "# vectorizer = CountVectorizer(tokenizer=test.tokenize_en)\n",
    "# transformed_data = vectorizer.fit_transform(new_text)\n",
    "# keyword_df_en = pd.DataFrame(columns = ['word', 'count'])\n",
    "# keyword_df_en['word'] = vectorizer.get_feature_names()\n",
    "# keyword_df_en['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "keyword_df_en = count_word_en(new_text_en)\n",
    "keyword_df_th = count_word_th(new_text_th)\n",
    "frames = [keyword_df_en, keyword_df_th]\n",
    "result = pd.concat(frames)\n",
    "\n",
    "topword = result.sort_values(by=['count'], ascending=False).head(20)\n",
    "topword\n",
    "# vectorizer.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e63482132cd177804ad55ee40df4b445c81008446e327e7d6aa26de5f74cb33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
